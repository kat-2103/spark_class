
# ‚ú® Proyecto: Ingesta, Transformaci√≥n y Visualizaci√≥n de Datos NYC Taxi Trips

## üìÑ Objetivo del Proyecto

Este proyecto tiene como fin que aprend√°is y praciqu√©is el ciclo completo de un flujo de datos ETL (Extracci√≥n, Transformaci√≥n y Carga) utilizando PySpark sobre Databricks Community Edition. Se trabajar√° con datos reales de viajes de taxi en Nueva York, disponibles de forma p√∫blica.
### Caso empresarial: 
Una empresa de transporte de taxis en Nueva York ha recopilado durante a√±os datos de sus viajes (fecha, ubicaci√≥n de recogida y destino, tarifas, n√∫mero de pasajeros, etc.). Actualmente, dispone de estos archivos en formato CSV y ha solicitado tu ayuda como ingeniero/a de datos para lograr lo siguiente:

1. Implementar un pipeline de datos que:
   * Ingesta autom√°ticamente nuevos archivos mensuales de viajes.
   * Procese y limpie los datos (fechas, ubicaciones, tarifas, pasajeros).
   * Agregue estad√≠sticas √∫tiles por hora o zona.
   * Almacene los resultados en un formato eficiente (Parquet o Delta).

2. Construir un dashboard visual con insights clave del negocio:
   * Horas con m√°s pasajeros.
   * Tarifas promedio.
   * Distancias recorridas.

3. Automatizar este flujo para que se ejecute al detectar nuevos archivos o en intervalos regulares.

üß† Aprendizajes del proyecto:

Crear y configurar clusters en Databricks.

Usar PySpark para cargar, transformar y agregar datos.

Almacenar resultados con Delta Lake.

Automatizar pipelines ETL incrementales.

Construir dashboards interactivos sobre notebooks.

---

# üè¢ Parte 1: Preparaci√≥n del Entorno

## ‚ñ∂ Qu√© es Databricks y por qu√© lo usamos

**Databricks** es una plataforma basada en la nube para el an√°lisis de datos y la ingenier√≠a de datos que combina Apache Spark con un entorno colaborativo de notebooks. La versi√≥n Community Edition es gratuita y perfecta para fines educativos.

### Ventajas:
- Ejecuta PySpark sin necesidad de configuraci√≥n local.
- Incluye almacenamiento temporal (DBFS).
- Permite crear dashboards y programar trabajos.

## ‚úÖ Registro y configuraci√≥n inicial

1. Crear cuenta en [https://community.cloud.databricks.com](https://community.cloud.databricks.com)
2. Crear un cluster:
   - Nombre: `nyc_taxi_cluster`
   - Runtime: usar el predeterminado.
   - Esperar a que est√© en estado RUNNING.

3. Crear un notebook:
   - Nombre: `NYC_Taxi_ETL`
   - Lenguaje: PySpark

---

# üöú Parte 2: Ingesta de Datos

## üìÉ Dataset: NYC Yellow Taxi Trips

- Fuente oficial: [NYC TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
- Ejemplo de archivo: `yellow_tripdata_2019-01.csv`

### Descarga e Ingesta de datos (c√≥digo explicado):

```python
# Descargar un archivo y moverlo a DBFS (Databricks File System)
import urllib.request
local_path = "/tmp/yellow_tripdata_2019-01.csv"
url = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.csv"
urllib.request.urlretrieve(url, local_path)

# Copiar al sistema distribuido de Databricks
dbutils.fs.cp("file:/tmp/yellow_tripdata_2019-01.csv", "dbfs:/tmp/nyc_taxi/yellow_tripdata_2019-01.csv")

# Leer CSV con Spark
df = spark.read.option("header", True).option("inferSchema", True).csv("dbfs:/tmp/nyc_taxi/yellow_tripdata_2019-01.csv")
```

**Explicaci√≥n paso a paso:**
- `urllib.request`: Esta l√≠nea descarga el archivo CSV desde la URL p√∫blica proporcionada (en este caso, desde los servidores de NYC Taxi Data) y lo guarda localmente en el entorno del notebook de Databricks. Este entorno local es ef√≠mero y se reinicia al cerrar el cluster.
- `dbutils.fs.cp`: Copiamos el archivo desde el sistema local (ruta file:/tmp/...) hacia el sistema de archivos distribuido propio de Databricks: DBFS (Databricks File System). Esto permite que el archivo est√© disponible de forma persistente en el entorno del workspace y pueda ser utilizado por m√∫ltiples notebooks.
                   Puedes ver los archivos que subes a DBFS navegando en el men√∫ lateral de Databricks:
                    Data > DBFS > Buscar dentro de /tmp/nyc_taxi/.
                    Tambi√©n puedes usar %fs ls /tmp/nyc_taxi/ desde una celda del notebook para ver los archivos.
- `spark.read.csv`: Esta l√≠nea utiliza PySpark para leer el archivo CSV desde la ubicaci√≥n en DBFS. Se especifican las opciones header=True (para que tome la primera l√≠nea como nombres de columnas) y inferSchema=True (para que intente detectar autom√°ticamente el tipo de dato de cada columna). El resultado se guarda como un DataFrame, que es la estructura principal de datos en PySpark para manipular y transformar datos a gran escala.

---

# üîß Parte 3: Transformaci√≥n de Datos

### Objetivos de la limpieza y transformaci√≥n:

- Convertir fechas en formato timestamp.
- Filtrar valores inv√°lidos (por ejemplo, pasajeros = 0).
- Crear nuevas columnas como la hora del viaje.
- **Dise√±ar las transformaciones necesarias seg√∫n el criterio del equipo de an√°lisis, con el fin de garantizar la calidad y utilidad de los datos para su representaci√≥n posterior.**

Estas transformaciones deben tener en cuenta que los datos resultantes alimentar√°n visualizaciones que ayudar√°n a la toma de decisiones. Por lo tanto, es importante asegurarse de que los datos est√©n completos, limpios y representen la realidad del negocio de forma comprensible. 

Adem√°s, **todas las decisiones de transformaci√≥n deben documentarse adecuadamente**, incluyendo el motivo de cada limpieza o agregado, para garantizar trazabilidad y transparencia del proceso ETL.

```python
from pyspark.sql.functions import col, to_timestamp, hour

# Limpieza b√°sica
df_clean = df.withColumn("pickup_datetime", to_timestamp(col("tpep_pickup_datetime")))              .withColumn("dropoff_datetime", to_timestamp(col("tpep_dropoff_datetime")))              .filter(col("pickup_datetime").isNotNull())              .filter(col("passenger_count") > 0)

# Nueva columna con la hora de recogida
df_clean = df_clean.withColumn("pickup_hour", hour("pickup_datetime"))
```

---

# üß∞ Parte 4: Agregaciones

### ¬øPor qu√© agregamos m√©tricas por hora?

La empresa cliente quiere comprender el comportamiento de los viajes de taxi a lo largo del d√≠a. Agregar los datos por hora permite identificar patrones como:
- Las horas con mayor n√∫mero de pasajeros.
- En qu√© momentos del d√≠a los viajes son m√°s largos o m√°s costosos.
- Qu√© franjas horarias generan m√°s ingresos.

Estas m√©tricas son clave para que el negocio tome decisiones operativas (por ejemplo, aumentar conductores en horas pico), estrat√©gicas (campa√±as de marketing en horarios espec√≠ficos) o incluso regulatorias (optimizar tarifas).

Este paso tambi√©n simplifica la visualizaci√≥n de los datos en el dashboard, ya que se reduce la granularidad sin perder informaci√≥n importante.

### Agregaci√≥n de m√©tricas por hora:
```python
agg_df = df_clean.groupBy("pickup_hour").agg(
    {"fare_amount": "avg", "trip_distance": "avg", "passenger_count": "sum"}
).withColumnRenamed("avg(fare_amount)", "avg_fare")  .withColumnRenamed("avg(trip_distance)", "avg_distance")  .withColumnRenamed("sum(passenger_count)", "total_passengers")
```

---

# üóÇ Parte 5: Almacenamiento en Delta Lake

## ‚ú® ¬øQu√© es Delta Lake y por qu√© usarlo?

**Delta Lake** es un motor de almacenamiento optimizado que se ejecuta sobre Apache Spark. Permite trabajar con datos estructurados y semi-estructurados de forma confiable y escalable. A diferencia de los formatos tradicionales como CSV o Parquet, Delta permite operaciones transaccionales (ACID) y mantenimiento incremental de los datos.

### Ventajas principales:
- Permite hacer `UPDATE`, `DELETE`, `MERGE`, lo que es esencial para mantener la consistencia cuando los datos cambian.
- Versionado de datos ("time travel"): puedes acceder al estado de una tabla en un punto anterior en el tiempo.
- Optimizaciones como `ZORDER` y `OPTIMIZE` para mejorar el rendimiento.
- Ideal para cargas incrementales (a√±adir solo nuevos datos sin sobrescribir lo anterior).

### ¬øD√≥nde ver las tablas Delta en Databricks?
- Las tablas Delta se guardan como carpetas con archivos `.parquet` m√°s un directorio especial `_delta_log`.
- Puedes explorar estos archivos con `%fs ls /tmp/delta/nyc_taxi_agg`.
- Tambi√©n puedes crear una tabla sobre una carpeta Delta con SQL:
```sql
CREATE TABLE taxi_agg
USING DELTA
LOCATION '/tmp/delta/nyc_taxi_agg';
```
Luego podr√°s consultar esta tabla con comandos SQL directamente desde el notebook o el panel de SQL en Databricks.

### Guardar como Delta:
```python
agg_df.write.format("delta").mode("overwrite").save("/tmp/delta/nyc_taxi_agg")
```

---

# üöÄ Parte 6: Ingesta de M√∫ltiples Archivos y Pipeline Incremental

## ¬øPor qu√© es importante esta parte?

El cliente ha indicado que genera mensualmente un archivo con los datos de viajes. En lugar de procesar todos los datos desde cero cada vez, lo ideal es tener un **pipeline incremental** que cargue √∫nicamente los nuevos archivos conforme est√©n disponibles. Esto ahorra tiempo, recursos y permite una automatizaci√≥n m√°s eficiente.

### Relaci√≥n con los objetivos del cliente:
- Permite actualizar el dashboard autom√°ticamente cuando llegan nuevos datos.
- Mejora la eficiencia operativa y reduce la posibilidad de errores por reprocesamiento completo.
- Sienta las bases para escalabilidad futura con a√±os completos de datos.

### ¬øC√≥mo se ve en Databricks?
- Los nuevos archivos se pueden subir manualmente a `/tmp/nyc_taxi/` o cargar desde una fuente externa (como S3 o Azure).
- Puedes ver los archivos en el explorador lateral (`Data > DBFS`) o ejecutar:
```python
%fs ls /tmp/nyc_taxi/
```
- Tambi√©n puedes crear l√≥gica en Python para detectar autom√°ticamente si hay archivos nuevos que a√∫n no se han procesado.

## Ingesta m√∫ltiple:
```python
df_all = spark.read.option("header", True).option("inferSchema", True).csv("dbfs:/tmp/nyc_taxi/yellow_tripdata_2019-*.csv")
```

## ¬øQu√© es un pipeline incremental y c√≥mo lo crear√°n los alumnos?

El pipeline incremental es una serie de pasos de procesamiento que los alumnos implementar√°n en un **notebook de PySpark**, con el objetivo de automatizar la carga de nuevos archivos de datos de taxi.

Este pipeline debe ejecutarse peri√≥dicamente o en funci√≥n de eventos (como la subida de un nuevo archivo) y realizar lo siguiente:

- Leer solo los archivos nuevos.
- Procesar y transformar los datos.
- Insertarlos en la tabla Delta existente sin duplicar ni sobreescribir datos previos.

Los estudiantes deber√°n dise√±ar este pipeline como parte de su entregable t√©cnico, integr√°ndolo en el flujo general del proyecto solicitado por el cliente.

## Pipeline incremental:
1. Detectar archivos nuevos.
2. Transformarlos.
3. Hacer `append` si ya existe una tabla Delta.

```python
df_new.write.format("delta").mode("append").save("/tmp/delta/nyc_taxi_agg")
```

---

# üìä Parte 7: Dashboards Interactivos

## Objetivo de esta secci√≥n

El cliente ha solicitado un **dashboard empresarial en Databricks** que resuma de forma visual y clara los indicadores clave de los viajes en taxi. Este dashboard debe permitir tomar decisiones estrat√©gicas y operativas a partir de datos reales, agregados y actualizados.

Los estudiantes deber√°n construir un **dashboard completo dentro de Databricks**, utilizando las herramientas gr√°ficas de la plataforma (no √∫nicamente `display(df)`), seleccionando los tipos de gr√°ficos apropiados (barras, l√≠neas, tablas) y organiz√°ndolos en un panel interactivo.

## Indicadores m√≠nimos a incluir:
- Promedio de tarifa por hora del d√≠a.
- Total de pasajeros por hora.
- Distancia media recorrida por hora.

## C√≥mo crearlo:
1. Seleccionar los resultados transformados desde un notebook.
2. En cada celda con una visualizaci√≥n, usar el bot√≥n **"+ Add to Dashboard"**.
3. Crear un nuevo dashboard llamado `NYC Taxi Dashboard`.
4. Agregar y organizar los gr√°ficos de manera clara y visualmente efectiva.
5. (Opcional) Incluir filtros interactivos si se crean tablas Delta con m√∫ltiples meses.

Este dashboard formar√° parte de la entrega al cliente y debe responder a las principales preguntas planteadas por ellos en el caso inicial.

---

# ‚è∞ Parte 8: Automatizaci√≥n con Triggers

## ¬øQu√© es un trigger y por qu√© es √∫til?

Un **trigger** (o desencadenador) permite ejecutar autom√°ticamente un notebook en Databricks sin intervenci√≥n manual. Esto es clave para el cliente, que necesita que los datos se procesen de forma peri√≥dica o cada vez que haya nueva informaci√≥n disponible.

La automatizaci√≥n permite que el dashboard est√© siempre actualizado con los √∫ltimos datos sin que nadie tenga que lanzar el proceso manualmente.

## C√≥mo automatizar la ejecuci√≥n en Databricks:

1. Ve al men√∫ lateral y abre la secci√≥n **Jobs**.
2. Pulsa en **Create Job**.
3. Asigna un nombre al job (por ejemplo: `nyc_taxi_etl_job`).
4. Selecciona el notebook que contiene tu pipeline como tarea principal.
5. Configura el cluster (puedes usar uno existente o permitir que Databricks lo cree temporalmente).

## Tipos de trigger:

- **Manual**: el job se ejecuta cuando t√∫ lo lances manualmente (√∫til para pruebas).
- **Programado**: el job se ejecuta autom√°ticamente seg√∫n una frecuencia que definas (cada d√≠a, cada hora, etc.).
- **Basado en condiciones**: con l√≥gica avanzada podr√≠as activar el job cuando se detecten nuevos archivos. Para esto necesitar√°s:
  - Guardar la lista de archivos procesados (por ejemplo en una tabla Delta).
  - Comparar esa lista con los archivos actuales en DBFS.
  - Si hay nuevos, ejecutar el pipeline.

Esto se puede hacer con un notebook de control y condicionales en Python.

Esta automatizaci√≥n es parte del entregable y garantiza que el sistema siga funcionando de forma aut√≥noma despu√©s de su implementaci√≥n.

---

# üîç Glosario R√°pido

- **Cluster**: Conjunto de m√°quinas que ejecutan Spark.
- **DBFS**: Sistema de archivos de Databricks.
- **ETL**: Extracci√≥n, Transformaci√≥n y Carga de datos.
- **Delta Lake**: Formato de almacenamiento optimizado con control transaccional.
- **Trigger**: Evento que lanza una tarea programada.

---

# üìà Anexo: Buenas pr√°cticas para la visualizaci√≥n de dashboards

Un dashboard bien dise√±ado debe ser claro, funcional y enfocado en las necesidades del usuario final. Aqu√≠ tienes algunas recomendaciones para construir un dashboard efectivo para el cliente:

### ‚úÖ Claridad visual
- Usa t√≠tulos descriptivos en cada gr√°fico.
- Agrupa visualizaciones relacionadas (por ejemplo, todas las m√©tricas horarias juntas).
- Usa colores coherentes y evita combinaciones confusas.

### üìä Elecci√≥n del gr√°fico adecuado
- Barras horizontales o verticales: ideales para comparar por horas o zonas.
- L√≠neas: para mostrar evoluci√≥n temporal.
- Tablas: para mostrar detalles espec√≠ficos como top zonas o valores exactos.

### üîç Enfoque en los KPIs del cliente
- No sobrecargues el dashboard: enf√≥cate en 3-5 indicadores clave que respondan a preguntas concretas del negocio.
- Usa t√≠tulos que respondan a una pregunta: "¬øA qu√© hora hay m√°s pasajeros?", "¬øCu√°ndo son m√°s largos los viajes?"

### üõ† Personalizaci√≥n t√©cnica en Databricks
- Usa "Add to Dashboard" solo para visualizaciones limpias y relevantes.
- Renombra cada visualizaci√≥n desde el panel del dashboard para facilitar su lectura.
- (Opcional) A√±ade filtros si has trabajado con m√∫ltiples meses o zonas.
