# ğŸš• PySpark - Procesamiento de Datos de Taxis en Databricks

## IntroducciÃ³n

Este documento describe el proceso de ingesta, limpieza, transformaciÃ³n, almacenamiento y anÃ¡lisis de datos de taxis de la ciudad de Nueva York en Databricks utilizando Auto Loader y Delta Lake. El objetivo del proyecto fue crear un flujo ETL eficiente que garantice la calidad de los datos y permita su anÃ¡lisis en tiempo real mediante visualizaciones interactivas.

Este proyecto incluye:

ğŸ§© Ingesta eficiente con Auto Loader
ğŸ§¼ Limpieza robusta y validaciÃ³n de datos
ğŸ”„ Transformaciones clave para el anÃ¡lisis
ğŸ’¾ Almacenamiento optimizado en Delta Lake
ğŸ“Š Visualizaciones para dashboards interactivos

## ğŸ“¥ 1. Ingesta de Datos con Auto Loader

Se utilizÃ³ Auto Loader de Databricks para detectar y cargar archivos CSV de forma incremental desde un volumen de almacenamiento:

```python
from pyspark.sql.types import *

# DefiniciÃ³n del esquema
schema = StructType([
    StructField("VendorID", IntegerType(), True),
    StructField("tpep_pickup_datetime", StringType(), True),
    StructField("tpep_dropoff_datetime", StringType(), True),
    StructField("passenger_count", IntegerType(), True),
    StructField("trip_distance", DoubleType(), True),
    StructField("RatecodeID", IntegerType(), True),
    StructField("store_and_fwd_flag", StringType(), True),
    StructField("PULocationID", IntegerType(), True),
    StructField("DOLocationID", IntegerType(), True),
    StructField("payment_type", IntegerType(), True),
    StructField("fare_amount", DoubleType(), True),
    StructField("extra", DoubleType(), True),
    StructField("mta_tax", DoubleType(), True),
    StructField("tip_amount", DoubleType(), True),
    StructField("tolls_amount", DoubleType(), True),
    StructField("improvement_surcharge", DoubleType(), True),
    StructField("total_amount", DoubleType(), True),
    StructField("congestion_surcharge", DoubleType(), True)
])

# Ruta de los datos
ruta = "/Volumes/workspace/default/data_taxi"

# Carga incremental con Auto Loader
raw_df = spark.readStream.format("cloudFiles") \
    .option("cloudFiles.format", "csv") \
    .option("header", "true") \
    .schema(schema) \
    .load(ruta)
```

## ğŸ§¹ 2. Limpieza de Datos

Se limpiaron los datos eliminando valores nulos, duplicados y registros invÃ¡lidos:

```python
from pyspark.sql.functions import col, to_timestamp

# ConversiÃ³n de columnas de fecha/hora
clean_df = raw_df.withColumn("tpep_pickup_datetime", to_timestamp("tpep_pickup_datetime")) \
                 .withColumn("tpep_dropoff_datetime", to_timestamp("tpep_dropoff_datetime"))

# EliminaciÃ³n de duplicados y valores nulos en columnas clave
clean_df = clean_df.dropna(subset=["tpep_pickup_datetime", "tpep_dropoff_datetime", "fare_amount"])
clean_df = clean_df.dropDuplicates()

# Filtro de valores invÃ¡lidos o inconsistentes
clean_df = clean_df.filter(
    (col("fare_amount") > 0) &
    (col("trip_distance") > 0) &
    (col("passenger_count") > 0)
)
```

## ğŸ”„ 3. TransformaciÃ³n de Datos

Se aÃ±adieron columnas para anÃ¡lisis posterior:

```python
from pyspark.sql.functions import hour, unix_timestamp

# Agregar columna con la hora de recogida
transformed_df = clean_df.withColumn("pickup_hour", hour("tpep_pickup_datetime"))

# Calcular duraciÃ³n del viaje en minutos
transformed_df = transformed_df.withColumn(
    "trip_duration_min",
    (unix_timestamp("tpep_dropoff_datetime") - unix_timestamp("tpep_pickup_datetime")) / 60
)
```

## ğŸ’¾ 4. Almacenamiento en Delta Lake

Los datos procesados fueron almacenados como tabla Delta administrada dentro del metastore de Databricks:

```python
# Guardar como tabla Delta en modo sobreescritura
transformed_df.write.format("delta").mode("overwrite").saveAsTable("taxi_data_bronze")
```

## ğŸ“ˆ 5. Visualizaciones Interactivas

Se realizaron varias visualizaciones clave en el notebook para generar un dashboard empresarial interactivo.

### ğŸš– 5.1 Promedio de Tarifa por Hora

```python
from pyspark.sql.functions import avg

df_avg_fare = transformed_df.groupBy("pickup_hour").agg(
    avg("fare_amount").alias("avg_fare")
).orderBy("pickup_hour")
```

### ğŸ‘¥ 5.2 Total de Pasajeros por Hora

```python
from pyspark.sql.functions import sum

df_total_passengers = transformed_df.groupBy("pickup_hour").agg(
    sum("passenger_count").alias("total_passengers")
).orderBy("pickup_hour")
```

### ğŸ“ 5.3 Distancia Media por Hora

```python
df_avg_distance = transformed_df.groupBy("pickup_hour").agg(
    avg("trip_distance").alias("avg_distance")
).orderBy("pickup_hour")
```

Estas mÃ©tricas fueron graficadas directamente en el notebook usando visualizaciones nativas de Databricks.

## âœ… ConclusiÃ³n

Este flujo ETL con PySpark y Delta Lake ofrece:

âœ¨ Alta eficiencia en la ingesta
ğŸ§ª Datos limpios y listos para anÃ¡lisis
ğŸ“Š Visualizaciones en tiempo real
ğŸ“š Fundamento sÃ³lido para futuras integraciones de BI

---

ğŸ“˜ Autor: Alexia Caride ğŸ‘¤