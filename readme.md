#  PySpark - Procesamiento de Datos de Taxis en Databricks

## Introducci贸n

Este documento describe el proceso de ingesta, limpieza, transformaci贸n, almacenamiento y an谩lisis de datos de taxis de la ciudad de Nueva York en Databricks utilizando Auto Loader y Delta Lake. El objetivo del proyecto fue crear un flujo ETL eficiente que garantice la calidad de los datos y permita su an谩lisis en tiempo real mediante visualizaciones interactivas.

Este proyecto incluye:

З Ingesta eficiente con Auto Loader
Ъ Limpieza robusta y validaci贸n de datos
 Transformaciones clave para el an谩lisis
 Almacenamiento optimizado en Delta Lake
 Visualizaciones para dashboards interactivos

##  1. Ingesta de Datos con Auto Loader

Se utiliz贸 Auto Loader de Databricks para detectar y cargar archivos CSV de forma incremental desde un volumen de almacenamiento:

```python
from pyspark.sql.types import *

# Definici贸n del esquema
schema = StructType([
    StructField("VendorID", IntegerType(), True),
    StructField("tpep_pickup_datetime", StringType(), True),
    StructField("tpep_dropoff_datetime", StringType(), True),
    StructField("passenger_count", IntegerType(), True),
    StructField("trip_distance", DoubleType(), True),
    StructField("RatecodeID", IntegerType(), True),
    StructField("store_and_fwd_flag", StringType(), True),
    StructField("PULocationID", IntegerType(), True),
    StructField("DOLocationID", IntegerType(), True),
    StructField("payment_type", IntegerType(), True),
    StructField("fare_amount", DoubleType(), True),
    StructField("extra", DoubleType(), True),
    StructField("mta_tax", DoubleType(), True),
    StructField("tip_amount", DoubleType(), True),
    StructField("tolls_amount", DoubleType(), True),
    StructField("improvement_surcharge", DoubleType(), True),
    StructField("total_amount", DoubleType(), True),
    StructField("congestion_surcharge", DoubleType(), True)
])

# Ruta de los datos
ruta = "/Volumes/workspace/default/data_taxi"

# Carga incremental con Auto Loader
raw_df = spark.readStream.format("cloudFiles") \
    .option("cloudFiles.format", "csv") \
    .option("header", "true") \
    .schema(schema) \
    .load(ruta)
```

## Ч 2. Limpieza de Datos

Se limpiaron los datos eliminando valores nulos, duplicados y registros inv谩lidos:

```python
from pyspark.sql.functions import col, to_timestamp

# Conversi贸n de columnas de fecha/hora
clean_df = raw_df.withColumn("tpep_pickup_datetime", to_timestamp("tpep_pickup_datetime")) \
                 .withColumn("tpep_dropoff_datetime", to_timestamp("tpep_dropoff_datetime"))

# Eliminaci贸n de duplicados y valores nulos en columnas clave
clean_df = clean_df.dropna(subset=["tpep_pickup_datetime", "tpep_dropoff_datetime", "fare_amount"])
clean_df = clean_df.dropDuplicates()

# Filtro de valores inv谩lidos o inconsistentes
clean_df = clean_df.filter(
    (col("fare_amount") > 0) &
    (col("trip_distance") > 0) &
    (col("passenger_count") > 0)
)
```

##  3. Transformaci贸n de Datos

Se a帽adieron columnas para an谩lisis posterior:

```python
from pyspark.sql.functions import hour, unix_timestamp

# Agregar columna con la hora de recogida
transformed_df = clean_df.withColumn("pickup_hour", hour("tpep_pickup_datetime"))

# Calcular duraci贸n del viaje en minutos
transformed_df = transformed_df.withColumn(
    "trip_duration_min",
    (unix_timestamp("tpep_dropoff_datetime") - unix_timestamp("tpep_pickup_datetime")) / 60
)
```

##  4. Almacenamiento en Delta Lake

Los datos procesados fueron almacenados como tabla Delta administrada dentro del metastore de Databricks:

```python
# Guardar como tabla Delta en modo sobreescritura
transformed_df.write.format("delta").mode("overwrite").saveAsTable("taxi_data_bronze")
```

##  5. Visualizaciones Interactivas

Se realizaron varias visualizaciones clave en el notebook para generar un dashboard empresarial interactivo.

###  5.1 Promedio de Tarifa por Hora

```python
from pyspark.sql.functions import avg

df_avg_fare = transformed_df.groupBy("pickup_hour").agg(
    avg("fare_amount").alias("avg_fare")
).orderBy("pickup_hour")
```

###  5.2 Total de Pasajeros por Hora

```python
from pyspark.sql.functions import sum

df_total_passengers = transformed_df.groupBy("pickup_hour").agg(
    sum("passenger_count").alias("total_passengers")
).orderBy("pickup_hour")
```

###  5.3 Distancia Media por Hora

```python
df_avg_distance = transformed_df.groupBy("pickup_hour").agg(
    avg("trip_distance").alias("avg_distance")
).orderBy("pickup_hour")
```

Estas m茅tricas fueron graficadas directamente en el notebook usando visualizaciones nativas de Databricks.

---

 Autor: Alexia Caride 
