{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bs4 in ./.venv/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.12/site-packages (from bs4) (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (4.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in ./.venv/lib/python3.12/site-packages (20.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: fastparquet in ./.venv/lib/python3.12/site-packages (2024.11.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in ./.venv/lib/python3.12/site-packages (from fastparquet) (2.2.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from fastparquet) (2.2.5)\n",
      "Requirement already satisfied: cramjam>=2.3 in ./.venv/lib/python3.12/site-packages (from fastparquet) (2.10.0)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from fastparquet) (2025.3.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from fastparquet) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "%pip install pandas \n",
    "%pip install bs4\n",
    "%pip install pyarrow\n",
    "%pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos y descargamos los archivos .parquet necesarios y los combinamos en un mismo archivo .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2020-01.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2020-02.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2020-03.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2020-04.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2020-05.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2020-06.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2020-07.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2020-08.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2020-09.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2020-10.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2020-11.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2020-12.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2021-01.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2021-02.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2021-03.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2021-04.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2021-05.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2021-06.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2021-07.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2021-08.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2021-09.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2021-10.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2021-11.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2021-12.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2022-01.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2022-02.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2022-03.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2022-04.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2022-05.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2022-06.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2022-07.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2022-08.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2022-09.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2022-10.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2022-11.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2022-12.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2023-01.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2023-02.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2023-03.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2023-04.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2023-06.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2023-12.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2024-04.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2024-05.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2024-06.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2024-07.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2024-08.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2024-09.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2024-10.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2024-11.parquet\n",
      "[SKIP] Ya existe: parquet_files/yellow_tripdata_2024-12.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2020-01.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2020-02.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2020-03.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2020-04.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2020-05.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2020-06.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2020-07.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2020-08.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2020-09.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2020-10.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2020-11.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2020-12.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2021-01.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2021-02.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2021-03.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2021-04.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2021-05.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2021-06.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2021-07.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2021-08.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2021-09.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2021-10.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2021-11.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2021-12.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2022-01.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2022-02.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2022-03.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2022-04.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2022-05.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2022-06.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2022-07.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2022-08.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2022-09.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2022-10.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2022-11.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2022-12.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2023-01.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2023-02.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2023-03.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2023-04.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2023-06.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2023-12.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2024-04.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2024-05.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2024-06.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2024-07.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2024-08.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2024-09.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2024-10.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2024-11.parquet\n",
      "Procesando parquet_files/yellow_tripdata_2024-12.parquet\n",
      "✅ Datos combinados guardados en: combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL base del portal de datos de taxis TLC NYC\n",
    "base_url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "# Rango de años a procesar\n",
    "years = range(2020, 2025)\n",
    "\n",
    "# Directorio de descarga y archivo de salida\n",
    "parquet_dir = \"parquet_files\"\n",
    "os.makedirs(parquet_dir, exist_ok=True)\n",
    "output_csv = \"combined_data.csv\"\n",
    "\n",
    "def download_parquet_files(year, url, taxi_type=\"yellow\"):\n",
    "    \"\"\"\n",
    "    Descarga archivos .parquet desde la página especificada según el año y tipo de taxi.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = soup.find_all(\"a\", href=lambda href: href and href.endswith(\".parquet\"))\n",
    "\n",
    "        for link in links:\n",
    "            if taxi_type in link[\"href\"] and str(year) in link[\"href\"]:\n",
    "                file_url = urljoin(url, link[\"href\"])\n",
    "                filename = os.path.join(parquet_dir, os.path.basename(file_url))\n",
    "\n",
    "                if os.path.exists(filename):\n",
    "                    print(f\"[SKIP] Ya existe: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"[DESCARGANDO] {file_url}\")\n",
    "                r = requests.get(file_url, stream=True)\n",
    "                with open(filename, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al acceder a {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado: {e}\")\n",
    "\n",
    "# Solicita al usuario el tipo de taxi\n",
    "taxi_type = input(\"¿Qué tipo de taxi quieres descargar? (yellow/green): \").strip().lower()\n",
    "\n",
    "# Ejecuta la descarga para cada año\n",
    "for year in years:\n",
    "    download_parquet_files(year, base_url, taxi_type)\n",
    "\n",
    "# Validación de columnas consistentes entre archivos\n",
    "columns_set = None\n",
    "valid_files = []\n",
    "for filename in os.listdir(parquet_dir):\n",
    "    if filename.endswith(\".parquet\"):\n",
    "        filepath = os.path.join(parquet_dir, filename)\n",
    "        try:\n",
    "            df = pd.read_parquet(filepath, engine=\"pyarrow\")\n",
    "            df.columns = [col.lower() for col in df.columns]\n",
    "            file_columns = set(df.columns)\n",
    "            del df\n",
    "\n",
    "            if columns_set is None:\n",
    "                columns_set = file_columns\n",
    "            elif file_columns != columns_set:\n",
    "                print(f\"⚠️ Columnas diferentes en: {filename}. Saltando este archivo.\")\n",
    "                continue\n",
    "\n",
    "            valid_files.append(filepath)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error al leer {filename}: {e}\")\n",
    "\n",
    "# Escritura del archivo CSV combinando los datos\n",
    "if valid_files:\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as f_out:\n",
    "        first = True\n",
    "        for filepath in sorted(valid_files):\n",
    "            print(f\"Procesando {filepath}\")\n",
    "            try:\n",
    "                df = pd.read_parquet(filepath, engine=\"pyarrow\")\n",
    "                df.to_csv(f_out, index=False, header=first)\n",
    "                first = False\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error al procesar {filepath}: {e}\")\n",
    "\n",
    "    print(f\"✅ Datos combinados guardados en: {output_csv}\")\n",
    "else:\n",
    "    print(\"⚠️ No se encontraron archivos .parquet válidos para procesar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos los datos en varios csv ya que databricks no permite subir archivos tan grandes, 2gb max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Guardado: csv_split/chunk_01.csv\n",
      "✅ Guardado: csv_split/chunk_02.csv\n",
      "✅ Guardado: csv_split/chunk_03.csv\n",
      "✅ Guardado: csv_split/chunk_04.csv\n",
      "✅ Guardado: csv_split/chunk_05.csv\n",
      "✅ Guardado: csv_split/chunk_06.csv\n",
      "✅ Guardado: csv_split/chunk_07.csv\n",
      "✅ Guardado: csv_split/chunk_08.csv\n",
      "✅ Guardado: csv_split/chunk_09.csv\n",
      "✅ Guardado final: csv_split/chunk_10.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Archivo CSV de entrada y directorio de salida\n",
    "input_csv = \"combined_data.csv\"\n",
    "output_dir = \"csv_split\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Número máximo de archivos de salida y tamaño objetivo por archivo (en bytes)\n",
    "max_chunks = 10\n",
    "target_chunk_size = os.path.getsize(input_csv) // max_chunks\n",
    "\n",
    "with open(input_csv, 'r', encoding='utf-8') as f_in:\n",
    "    header = f_in.readline()  # Leer encabezado una vez\n",
    "    chunk_idx = 1\n",
    "    chunk_lines = []\n",
    "    current_chunk_size = 0\n",
    "\n",
    "    # Leer línea por línea y dividir según tamaño aproximado\n",
    "    for line in f_in:\n",
    "        chunk_lines.append(line)\n",
    "        current_chunk_size += len(line.encode('utf-8'))\n",
    "\n",
    "        if current_chunk_size >= target_chunk_size:\n",
    "            chunk_path = os.path.join(output_dir, f\"chunk_{chunk_idx:02}.csv\")\n",
    "            with open(chunk_path, 'w', encoding='utf-8') as f_out:\n",
    "                f_out.write(header)\n",
    "                f_out.writelines(chunk_lines)\n",
    "            print(f\"✅ Guardado: {chunk_path}\")\n",
    "\n",
    "            chunk_lines = []\n",
    "            current_chunk_size = 0\n",
    "            chunk_idx += 1\n",
    "\n",
    "    # Guardar cualquier contenido restante como último archivo\n",
    "    if chunk_lines:\n",
    "        chunk_path = os.path.join(output_dir, f\"chunk_{chunk_idx:02}.csv\")\n",
    "        with open(chunk_path, 'w', encoding='utf-8') as f_out:\n",
    "            f_out.write(header)\n",
    "            f_out.writelines(chunk_lines)\n",
    "        print(f\"✅ Guardado final: {chunk_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
