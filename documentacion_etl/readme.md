# ğŸš— Proyecto ETL: AnÃ¡lisis de Viajes en Taxi de Nueva York con PySpark

## ğŸ“Œ IntroducciÃ³n

Este proyecto tiene como objetivo realizar un proceso completo de ETL (ExtracciÃ³n, TransformaciÃ³n y Carga) usando PySpark sobre la plataforma Databricks. Se trabajan datos reales de taxis en Nueva York para analizarlos y visualizarlos de forma interactiva.

## ğŸ“‚ 1. Carga de Datos

Primero accedÃ­ a los archivos CSV almacenados en el volumen de trabajo de Databricks:

```python
ruta_volumen = "/Volumes/workspace/default/data_taxi"
archivos = dbutils.fs.ls(ruta_volumen)
archivos_csv = [archivo.path for archivo in archivos if archivo.path.endswith(".csv")]
df = spark.read.option("header", True).option("inferSchema", True).csv(ruta_volumen)

```

Con esto pude cargar todos los archivos disponibles en un solo DataFrame para trabajar de forma unificada.

## ğŸ§¹ 2. Limpieza y TransformaciÃ³n de Datos

TransformÃ© el DataFrame original para dejar solo los datos vÃ¡lidos:

```python
from pyspark.sql.functions import to_timestamp, hour, col

df_clean = (
    df.withColumn("pickup_datetime", to_timestamp(col("tpep_pickup_datetime")))
      .withColumn("dropoff_datetime", to_timestamp(col("tpep_dropoff_datetime")))
      .filter(col("pickup_datetime").isNotNull())
      .filter(col("dropoff_datetime").isNotNull())
      .filter(col("passenger_count") > 0)
      .filter(col("trip_distance") > 0)
      .filter(col("fare_amount") >= 0)
      .withColumn("pickup_hour", hour(col("pickup_datetime")))
)

df_clean = df_clean.drop("airport_fee", "store_and_fwd_flag")
```

Esto me permitiÃ³:

Convertir las fechas a formato timestamp

Eliminar registros con valores errÃ³neos o vacÃ­os

Crear una columna para la hora de recogida

Quitar columnas innecesarias

## âŒ 3. Tratamiento de Valores Nulos

Para asegurarme de trabajar con datos completos, eliminÃ© los registros con valores nulos en las columnas clave:

from pyspark.sql.functions import to_timestamp, hour, col
```python
# Limpieza y transformaciÃ³n
df_clean = (
    df.withColumn("pickup_datetime", to_timestamp(col("tpep_pickup_datetime")))
    .withColumn("dropoff_datetime", to_timestamp(col("tpep_dropoff_datetime")))
    .filter(col("pickup_datetime").isNotNull())
    .filter(col("dropoff_datetime").isNotNull())
    .filter(col("passenger_count") > 0)
    .filter(col("trip_distance") > 0)
    .filter(col("fare_amount") >= 0)
    .withColumn("pickup_hour", hour(col("pickup_datetime")))
)

# Ver algunos registros limpios
display(
    df_clean.select(
        "pickup_datetime",
        "pickup_hour",
        "passenger_count",
        "trip_distance",
        "fare_amount",
    )
)
```
## ğŸ“Š 4. AgregaciÃ³n de Datos por Hora

AgrupÃ© los viajes por hora para obtener mÃ©tricas importantes del negocio:
```python
agg_df = (
    df_clean.groupBy("pickup_hour")
    .agg({
        "fare_amount": "avg",
        "trip_distance": "avg",
        "passenger_count": "sum"
    })
    .withColumnRenamed("avg(fare_amount)", "avg_fare")
    .withColumnRenamed("avg(trip_distance)", "avg_distance")
    .withColumnRenamed("sum(passenger_count)", "total_passengers")
)
```
Esto me permitiÃ³ analizar quÃ© horas concentran mÃ¡s actividad, cuÃ¡ndo los trayectos son mÃ¡s largos o cuÃ¡ndo se generan mayores ingresos.

## ğŸ“€ 5. Almacenamiento en Delta Lake

Para almacenar los resultados de forma eficiente, utilicÃ© formato Delta:
```python
df_clean.write.format("delta").mode("overwrite").saveAsTable("df_clean_delta")
agg_df.write.format("delta").mode("overwrite").saveAsTable("agg_df_delta")
```

## ğŸ“ˆ 6. VisualizaciÃ³n de Datos en Dashboards

CreÃ© dashboards con las siguientes visualizaciones:

**ğŸ“† a. Total de pasajeros por hora**

```python
from pyspark.sql.functions import hour

df_con_hora = df.withColumn("hora", hour("tpep_pickup_datetime"))
df_total_pasajeros = df_con_hora.groupBy("hora").count().alias("total_pasajeros")
display(df_total_pasajeros)
```

**ğŸ’¸ b. Tarifa promedio por hora**

```python
from pyspark.sql.functions import avg

df_promedio_tarifa = df_con_hora.groupBy("hora").agg(avg("total_amount").alias("promedio_tarifa"))
display(df_promedio_tarifa)
```

**ğŸŒ c. Distancia media por hora**

```python
df_distancia_media = df_con_hora.groupBy("hora").agg(avg("trip_distance").alias("distancia_media"))
display(df_distancia_media)
```

## ğŸ“Š ConclusiÃ³n

Este proyecto me ha ayudado a aprender a usar PySpark para analizar datos reales en Databricks. He practicado el ciclo completo de ETL y he creado dashboards con KPIs que podrÃ­an servir para tomar decisiones en una empresa de taxis. Aunque algunas partes han sido desafiantes, he aprendido un montÃ³n y me siento mucho mÃ¡s segura trabajando con datos âœ¨ğŸ’»
