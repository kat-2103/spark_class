# üöñ An√°lisis de Datos de Taxis Amarillos en NYC (2020‚Äì2024)

## üìå Descripci√≥n del Proyecto

Este proyecto analiza los datos de los taxis amarillos en Nueva York entre los a√±os 2020 y 2024. A trav√©s de ETL desarrollado con Apache Spark y visualizaciones en Lightdash, se estudian los patrones de uso, ingresos y comportamiento horario del servicio de taxis.

## üóÉÔ∏è Fuentes de Datos

Los datos provienen del portal oficial de taxis de NYC: [NYC Taxi & Limousine Commission](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

- Los archivos fueron guardados en **Google Drive** y posteriormente montados en un volumen manual llamado `test_volume` dentro del entorno de Databricks.

## ‚öôÔ∏è Proceso ETL (Extract, Transform, Load)

El proceso se llev√≥ a cabo en un notebook de Jupyter usando PySpark para manejar grandes vol√∫menes de datos.

### 1. Lectura de Datos

Inicialmente intent√© automatizar la descarga de los archivos desde la web oficial de NYC Taxi y copiarlos a una carpeta temporal (`/tmp`) en el entorno de Databricks, pero me encontr√© con problemas de permisos.

Como soluci√≥n alternativa:

- Subieron los archivos a [**Google Drive**](https://drive.google.com/file/d/1KH8jxRvZ9Z5AesszU1ZA0HHiA7RsxaWR/view).
- Desde all√≠, los transfer√≠ manualmente al volumen montado en Databricks llamado `test_volume`.
- Finalmente, los le√≠ directamente desde este volumen usando PySpark y los almacen√© en un **DataFrame** para continuar con el proceso de transformaci√≥n.
### 2. Limpieza y Manejo de Nulos

Durante la limpieza, se identificaron varias columnas con valores nulos:

- `passenger_count`
- `ratecodeID`
- `store_and_fwd_flag`
- `congestion_surcharge`
- `airport_fee`

Las decisiones fueron:

- ‚úÖ **`passenger_count`**: Se filtraron los registros con valores inv√°lidos (`passenger_count = 0`) y se eliminaron las filas con valores nulos.
- ‚ùå **Otras columnas**: Se eliminaron las columnas `ratecodeID`, `store_and_fwd_flag`, `congestion_surcharge` y `airport_fee` ya que no aportaban valor para el an√°lisis planteado y conten√≠an muchos nulos. Esta decisi√≥n tambi√©n se bas√≥ en los requisitos y preguntas de negocio definidos.

Adem√°s, al realizar un `display()` para visualizar el n√∫mero de pasajeros por **a√±o, mes y hora**, detect√© que exist√≠an registros fuera del rango de a√±os 2020‚Äì2024. Para asegurar la coherencia temporal del an√°lisis, **elimin√© todos los registros correspondientes a a√±os fuera de este intervalo.**

```python
df = df.filter(col("passenger_count") > 0)
columnas_a_eliminar = ["airport_fee", "congestion_surcharge", "RatecodeID", "store_and_fwd_flag"] 
df = df.drop(*columnas_a_eliminar)
````

### 3. Conversi√≥n de Timestamps

Se transformaron las columnas de tiempo:

- `tpep_pickup_datetime`
- `tpep_dropoff_datetime`

Estas se convirtieron a tipo `datetime` para permitir operaciones de agregaci√≥n temporal.
```python
from pyspark.sql.functions import to_timestamp

df = df.withColumn("tpep_pickup_datetime", to_timestamp(df["tpep_pickup_datetime"], "yyyy-MM-dd HH:mm:ss"))
df = df.withColumn("tpep_dropoff_datetime", to_timestamp(df["tpep_dropoff_datetime"], "yyyy-MM-dd HH:mm:ss"))
```

### 4. Particionamiento de Datos

Para optimizar el rendimiento de las consultas, los datos se particionaron por:

- `hour` (hora de recogida): muchas de las consultas agregadas se hacen por hora.
- `year`, `month`, `day`: para permitir filtrado r√°pido por periodos de tiempo.

Esta decisi√≥n mejora significativamente los tiempos de respuesta en entornos distribuidos como Spark o cuando se consulta desde herramientas como Lightdash.
```python
from pyspark.sql.functions import hour

df_con_hora = df.withColumn("hour", hour("tpep_pickup_datetime"))

from pyspark.sql.functions import year, month, dayofmonth

df = df_con_hora \
    .withColumn("year", year("tpep_pickup_datetime")) \
    .withColumn("month", month("tpep_pickup_datetime")) \
    .withColumn("day", dayofmonth("tpep_pickup_datetime"))
```

## üìä Visualizaci√≥n (Lightdash)

La visualizaci√≥n se desarroll√≥ en Lightdash, conectando directamente con las tablas creadas en Databricks. Se construyeron dashboards interactivos con insights como:

- Pasajeros por hora y mes, a√±o a a√±o (2020‚Äì2024).
- Ingresos totales por franjas horarias.
- Distancia media y tarifa media seg√∫n la hora del d√≠a.

Estas visualizaciones permiten detectar patrones de demanda y comportamiento del servicio, como las **horas pico**, o las franjas horarias que generan m√°s ingresos.

## üß™ An√°lisis Exploratorio

Durante el an√°lisis, se realizaron varias agregaciones clave para entender los patrones de comportamiento de los viajes en NYC.

### üîπ Horas con mayor n√∫mero de pasajeros

```python
from pyspark.sql.functions import sum

hour_passengers = df.select("hour", "passenger_count") \
  .groupBy("hour") \
  .agg(sum("passenger_count").alias("total_passengers")) \
  .orderBy("total_passengers", ascending=False)

display(hour_passengers)
```

### üîπ Horas con mayor n√∫mero de pasajeros

En este an√°lisis agrupamos por **hora del d√≠a** y sumamos la cantidad total de pasajeros para identificar las **horas pico con mayor demanda**.

```python
from pyspark.sql.functions import sum

hour_passengers = df.select("hour", "passenger_count") \
  .groupBy("hour") \
  .agg(sum("passenger_count").alias("total_passengers")) \
  .orderBy("total_passengers", ascending=False)

display(hour_passengers)
```
![Horas con mayor n√∫mero de pasajeros 2020-2024](https://github.com/user-attachments/assets/31c0f2dd-ff1a-485f-a153-4b6d9b5bf6e0)


### üîπ  Momentos del d√≠a con viajes m√°s largos o caros

Aqu√≠ calculamos la **distancia media** y la **tarifa media** por hora. Esto nos permite identificar qu√© horas concentran los **viajes m√°s largos y costosos**.

```python
from pyspark.sql.functions import avg

df_largos_caros = df.select("hour", "trip_distance", "fare_amount") \
  .groupBy("hour") \
  .agg(
      avg("trip_distance").alias("avg_distance"),
      avg("fare_amount").alias("avg_fare")
  ) \
  .orderBy("avg_distance", ascending=False)

display(df_largos_caros)
```
![Tarifa media de cada hora](https://github.com/user-attachments/assets/0f99a173-a9eb-459a-b655-2ea5cc241e05)
![Distancia media por hora](https://github.com/user-attachments/assets/4ee82f73-edbf-4fd5-bb8b-1191d21b58fa)


### üîπ   Franjas horarias que generan m√°s ingresos

Sumamos el **total de ingresos por hora** para detectar en qu√© momentos del d√≠a se genera **mayor rentabilidad para los taxistas**.


```python
from pyspark.sql.functions import sum

df_ingresos = df.select("hour", "fare_amount") \
  .groupBy("hour") \
  .agg(sum("fare_amount").alias("total_revenue")) \
  .orderBy("total_revenue", ascending=False)

display(df_ingresos)
```
![Franjas horarias que general m√°s ingresos 2020-2024](https://github.com/user-attachments/assets/834a182a-d72b-4a80-afe5-436b7b9f1755)

