# Proyecto: Ingesta, Transformaci√≥n y Visualizaci√≥n de Datos NYC Taxi Trips con PySpark en Google Colab

## üìÑ Descripci√≥n General

Este proyecto demuestra un ciclo completo de un flujo de datos ETL (Extracci√≥n, Transformaci√≥n y Carga) utilizando PySpark en el entorno de Google Colab. El objetivo principal es proporcionar una gu√≠a pr√°ctica para trabajar con datos de viajes de taxi de Nueva York, desde la ingesta de archivos Parquet hasta la visualizaci√≥n de insights clave. Est√° dise√±ado para que los usuarios puedan aprender y practicar conceptos fundamentales de procesamiento de datos a gran escala con Spark.

### üéØ Objetivos del Proyecto

*   Implementar un pipeline de datos para ingestar, procesar y limpiar datos de viajes de taxi.
*   Realizar agregaciones para obtener estad√≠sticas √∫tiles (ej. viajes por hora, tarifas promedio).
*   Almacenar los datos procesados y agregados en formato Parquet.
*   Construir visualizaciones para extraer insights del negocio directamente en el notebook.
*   Comprender c√≥mo configurar y utilizar PySpark en Google Colab.

### üíº Caso Empresarial Simulado

Una empresa de transporte de taxis en Nueva York busca modernizar su an√°lisis de datos. Han recopilado datos de viajes durante a√±os y necesitan un sistema para:

1.  **Ingesta Automatizada:** Procesar nuevos archivos mensuales de datos de viajes.
2.  **Limpieza y Transformaci√≥n:** Estandarizar formatos de fecha, manejar datos faltantes, calcular duraciones de viaje, etc.
3.  **Agregaci√≥n de Insights:** Identificar horas pico de demanda, tarifas promedio por zona, patrones de distancia, etc.
4.  **Almacenamiento Eficiente:** Guardar los datos limpios y agregados en un formato optimizado como Parquet.
5.  **Visualizaci√≥n:** Crear dashboards o reportes visuales para la toma de decisiones.

Este notebook aborda estos puntos, adaptando la soluci√≥n al entorno de Google Colab.

### üß† Aprendizajes Clave

Al completar este proyecto, se espera que el usuario pueda:

*   Configurar e inicializar una sesi√≥n de PySpark en Google Colab.
*   Cargar datos desde m√∫ltiples archivos Parquet en un DataFrame de Spark.
*   Aplicar diversas transformaciones de datos: limpieza, conversi√≥n de tipos, ingenier√≠a de caracter√≠sticas (extracci√≥n de componentes de fecha, c√°lculo de duraciones).
*   Realizar agregaciones de datos utilizando operaciones `groupBy`.
*   Guardar DataFrames de Spark en formato Parquet.
*   Convertir DataFrames de Spark a Pandas para visualizaci√≥n.
*   Crear visualizaciones b√°sicas utilizando librer√≠as como Matplotlib, Seaborn o Plotly.
*   Comprender las consideraciones al trabajar con m√∫ltiples archivos y esquemas potencialmente diferentes.

## üõ†Ô∏è Requisitos y Dependencias

*   **Entorno:** Google Colab (o cualquier entorno Python con Jupyter Notebooks y Spark instalado).
*   **Lenguaje:** Python 3.x
*   **Librer√≠as Principales:**
    *   `pyspark`: Para el procesamiento distribuido de datos.
    *   `findspark`: Para ayudar a localizar Spark en el sistema.
*   **Librer√≠as de Soporte (generalmente preinstaladas en Colab o instaladas con PySpark):**
    *   `os`: Para operaciones del sistema operativo (manejo de rutas).
    *   `urllib.request`: Para descargar archivos desde URLs.
    *   `json`: Para manejar datos en formato JSON (si se usa para listas de archivos).
*   **Librer√≠as de Visualizaci√≥n (ejemplos en el notebook):**
    *   `matplotlib`
    *   `seaborn`
    *   `plotly`

### Instalaci√≥n en Google Colab

La primera celda de c√≥digo en el notebook se encarga de instalar `pyspark` y `findspark`:

```python
!pip install -q pyspark findspark
```

## üìÇ Estructura del Repositorio (Sugerida)

```
/nyc-taxi-etl-colab
|-- NYC_Taxi_ETL_Colab.ipynb  # El notebook principal del proyecto
|-- sample.md                 # Esta documentaci√≥n (README)
|-- parquet_links.json        # (Opcional) Archivo JSON con URLs a los datasets Parquet de NYC TLC
|-- output_data/              # (Directorio creado al ejecutar) Para almacenar los Parquet procesados
|   |-- processed_trips.parquet
|   |-- hourly_pickups.parquet
|   |-- ... (otros resultados agregados)
|-- images/                   # (Opcional) Para cualquier imagen utilizada en esta documentaci√≥n
```

## üöÄ Instrucciones de Uso

1.  **Abrir el Notebook:**
    *   Sube el archivo `NYC_Taxi_ETL_Colab.ipynb` a tu entorno de Google Colab.
    *   Alternativamente, si est√° en un repositorio Git, puedes abrirlo directamente en Colab usando la URL del archivo en GitHub.

2.  **Preparar el Entorno:**
    *   Ejecuta la primera celda de c√≥digo para instalar `pyspark` y `findspark`.
    *   Ejecuta la celda que inicializa `findspark` y crea la `SparkSession`. Esto prepara el entorno Spark para su uso.

3.  **Configurar Fuentes de Datos (Ingesta):
    *   Localiza la celda de c√≥digo bajo la secci√≥n "üöú Parte 2: Ingesta de Datos".
    *   Dentro de esta celda, encontrar√°s una lista llamada `parquet_files_to_download` (o similar).
    *   **Modifica esta lista** para incluir las URLs de los archivos Parquet de NYC Taxi que deseas procesar. Puedes a√±adir o quitar diccionarios de la lista. Cada diccionario debe tener al menos una clave `"href"` con la URL del archivo Parquet.
        ```python
        # Ejemplo de c√≥mo modificar la lista:
        parquet_files_to_download = [
            {"text": "Yellow Taxi Trip Records 2023-01", "href": "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet"},
            {"text": "Yellow Taxi Trip Records 2023-02", "href": "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet"},
            # A√±ade m√°s archivos aqu√≠ si es necesario
        ]
        ```
    *   (Opcional) Si tienes un archivo `parquet_links.json` con una lista completa de URLs, puedes adaptar el c√≥digo para leer este JSON y seleccionar los archivos desde all√≠.

4.  **Ejecutar el Pipeline ETL:**
    *   Ejecuta las celdas del notebook en secuencia.
    *   **Ingesta:** Los archivos Parquet especificados se descargar√°n al directorio `/content/data/` en Colab y luego se cargar√°n en un DataFrame de Spark.
    *   **Transformaci√≥n:** Se aplicar√°n diversas operaciones de limpieza y transformaci√≥n de datos.
    *   **Agregaci√≥n:** Se calcular√°n estad√≠sticas y se agrupar√°n los datos.
    *   **Almacenamiento:** Los DataFrames resultantes (procesados y agregados) se guardar√°n en formato Parquet en el directorio `/content/output_data/` (o el especificado en el c√≥digo).
    *   **Visualizaci√≥n:** Se generar√°n gr√°ficos directamente en las celdas de salida del notebook.

5.  **Revisar Resultados:**
    *   **Archivos Parquet:** Puedes encontrar los archivos Parquet generados en el panel de archivos de Colab, dentro de la carpeta `output_data`. Puedes descargarlos a tu m√°quina local.
    *   **Visualizaciones:** Observa los gr√°ficos generados en el notebook para entender los insights de los datos.
    *   **Logs y Salidas:** Revisa las salidas de las celdas para ver esquemas de DataFrames, conteos de filas y cualquier mensaje de log.

## ‚öôÔ∏è Flujo de Trabajo Detallado (Resumen del Notebook)

El notebook est√° estructurado en varias partes l√≥gicas:

### Parte 1: Preparaci√≥n del Entorno
*   Instalaci√≥n de dependencias (`pyspark`, `findspark`).
*   Inicializaci√≥n de la `SparkSession`:
    ```python
    import findspark
    findspark.init()
    from pyspark.sql import SparkSession

    spark = SparkSession.builder \
        .appName("NYCTaxiETLColab") \
        .master("local[*]") \
        .getOrCreate()
    sc = spark.sparkContext
    ```

### Parte 2: Ingesta de Datos
*   Definici√≥n de las URLs de los archivos Parquet a procesar.
*   Descarga de los archivos al entorno local de Colab (`/content/data/`).
*   Lectura de m√∫ltiples archivos Parquet en un √∫nico DataFrame de Spark. Spark puede manejar la lectura de m√∫ltiples archivos y, si los esquemas son compatibles, los unir√°. Si los esquemas difieren (ej. al mezclar datos de taxis amarillos y verdes), Spark intentar√° una "fusi√≥n de esquemas".
    ```python
    # `downloaded_file_paths` es una lista de rutas locales a los archivos descargados
    # ej: ["/content/data/yellow_tripdata_2023-01.parquet", ...]
    uri_file_paths = [f"file://{path}" for path in downloaded_file_paths]
    df = spark.read.parquet(*uri_file_paths)
    
    df.printSchema()
    df.show(5)
    ```

### Parte 3: Limpieza y Transformaci√≥n de Datos
Esta secci√≥n incluye varias sub-tareas comunes en un ETL:
*   **Conversi√≥n de Tipos de Datos:** Asegurar que las columnas tengan los tipos correctos (ej. fechas a `TimestampType`, num√©ricos a `DoubleType` o `IntegerType`).
*   **Manejo de Nulos:** Identificar y tratar valores faltantes (ej. eliminar filas o imputar valores).
*   **Renombrar Columnas:** Para mayor claridad o consistencia.
*   **Ingenier√≠a de Caracter√≠sticas:** Crear nuevas columnas √∫tiles para el an√°lisis. Ejemplos:
    *   Extraer componentes de fecha/hora (a√±o, mes, d√≠a, hora, d√≠a de la semana) de las columnas de timestamp.
        ```python
        from pyspark.sql.functions import year, month, dayofmonth, hour, dayofweek
        df = df.withColumn("pickup_hour", hour("tpep_pickup_datetime"))
        ```
    *   Calcular la duraci√≥n del viaje.
        ```python
        from pyspark.sql.functions import col
        df = df.withColumn("trip_duration_seconds", 
                           (col("tpep_dropoff_datetime").cast("long") - col("tpep_pickup_datetime").cast("long")))
        df = df.withColumn("trip_duration_minutes", col("trip_duration_seconds") / 60)
        ```
*   **Filtrado de Datos:** Eliminar registros an√≥malos o irrelevantes (ej. viajes con distancia o tarifa cero/negativa, n√∫mero de pasajeros inv√°lido).
    ```python
    df_filtered = df.filter((col("trip_distance") > 0) & 
                            (col("fare_amount") > 0) & 
                            (col("passenger_count") > 0))
    ```

### Parte 4: Agregaci√≥n de Datos
*   Agrupar datos para obtener estad√≠sticas resumidas. Ejemplo: contar viajes por hora.
    ```python
    from pyspark.sql.functions import desc
    hourly_pickups = df_filtered.groupBy("pickup_hour").count().orderBy(desc("count"))
    hourly_pickups.show()
    ```
*   Otros ejemplos pueden incluir tarifas promedio por zona, distancia promedio por tipo de pasajero, etc.

### Parte 5: Almacenamiento de Datos
*   Guardar los DataFrames procesados y/o agregados en formato Parquet para uso futuro o an√°lisis m√°s profundo.
    ```python
    df_filtered.write.mode("overwrite").parquet("/content/output_data/processed_taxi_trips.parquet")
    hourly_pickups.write.mode("overwrite").parquet("/content/output_data/hourly_pickups_summary.parquet")
    ```
    *   `mode("overwrite")` reemplazar√° los archivos si ya existen. Otras opciones son `append`, `ignore`, `errorifexists`.

### Parte 6: Visualizaci√≥n de Datos
*   Convertir los DataFrames de Spark (que suelen ser agregados y de tama√±o manejable) a DataFrames de Pandas para facilitar la visualizaci√≥n con librer√≠as est√°ndar de Python.
    ```python
    pandas_hourly_pickups = hourly_pickups.toPandas()
    ```
*   Crear gr√°ficos utilizando `matplotlib.pyplot`, `seaborn`, o `plotly.express`.
    *   Ejemplo con Matplotlib/Seaborn para un gr√°fico de barras:
        ```python
        import matplotlib.pyplot as plt
        import seaborn as sns

        plt.figure(figsize=(12, 6))
        sns.barplot(x="pickup_hour", y="count", data=pandas_hourly_pickups, palette="viridis")
        plt.title("N√∫mero de Viajes de Taxi por Hora del D√≠a")
        plt.xlabel("Hora de Recogida")
        plt.ylabel("N√∫mero de Viajes")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
        ```

## üí° Consideraciones Adicionales

*   **Manejo de Esquemas al Cargar M√∫ltiples Archivos:**
    *   Si cargas archivos Parquet de diferentes tipos de datos (ej. Yellow Taxi, Green Taxi, FHV) simult√°neamente, sus esquemas pueden diferir (diferentes nombres de columna, tipos de datos, columnas adicionales/faltantes).
    *   PySpark intentar√° fusionar estos esquemas si lees desde un directorio (`spark.read.option("mergeSchema", "true").parquet(directorio)`). Si lees una lista de archivos, el comportamiento puede variar o requerir uni√≥n manual si los esquemas son muy dispares.
    *   **Es crucial inspeccionar el esquema del DataFrame combinado (`df.printSchema()`)** despu√©s de la carga y adaptar las transformaciones subsiguientes. Es posible que necesites manejar columnas que solo existen en algunos de los datasets fuente (resultando en nulos para otros) o estandarizar nombres de columnas.

*   **Recursos en Google Colab:**
    *   Las sesiones gratuitas de Google Colab tienen limitaciones en cuanto a RAM, disco y tiempo de ejecuci√≥n. El procesamiento de vol√∫menes muy grandes de datos puede exceder estos l√≠mites.
    *   Para datasets m√°s grandes, considera ejecutar el notebook en lotes m√°s peque√±os, muestrear los datos o utilizar una plataforma Spark m√°s robusta.

*   **Persistencia de Datos:**
    *   Los archivos guardados en el sistema de archivos de Colab (`/content/`) son temporales y se eliminan cuando la sesi√≥n termina.
    *   Para persistir tus datos de entrada, salida o el propio notebook, monta tu Google Drive y ajusta las rutas de lectura/escritura para que apunten a tu Drive.
        ```python
        from google.colab import drive
        drive.mount('/content/drive')
        # Ejemplo de ruta en Drive: "/content/drive/MyDrive/Colab Notebooks/NYCTaxi/output_data/"
        ```

*   **Automatizaci√≥n:**
    *   La automatizaci√≥n completa de pipelines ETL como se har√≠a en un entorno de producci√≥n (ej. con Apache Airflow, cron jobs en un servidor, o Databricks Jobs) tiene limitaciones en Colab.
    *   Colab no est√° dise√±ado para ser un orquestador de flujos de trabajo productivos. Sin embargo, puedes:
        *   Ejecutar el notebook manualmente cuando sea necesario.
        *   Explorar opciones limitadas de ejecuci√≥n programada si Google ofrece alguna funcionalidad para ello (esto puede cambiar).
        *   Utilizar APIs de Colab si deseas activar la ejecuci√≥n desde un script externo, aunque esto es m√°s complejo.

## ü§ù Contribuciones

Las contribuciones a este proyecto son bienvenidas. Por favor, abre un *issue* para discutir cambios importantes o env√≠a un *pull request* con tus mejoras.

## üìú Licencia

Este proyecto se distribuye bajo la Licencia MIT. Consulta el archivo `LICENSE` para m√°s detalles (si aplica).

