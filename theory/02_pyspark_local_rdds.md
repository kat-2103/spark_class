# ğŸ“‚ 02 - PySpark en Local: Primeros pasos con RDDs

---

# Objetivo

Crear, transformar y ejecutar acciones sobre un **RDD** en PySpark, trabajando **en local**.

---

# Conceptos clave

- **RDD (Resilient Distributed Dataset)**: estructura bÃ¡sica de Spark, una colecciÃ³n distribuida de objetos.
- **Transformaciones**: operaciones como `map()`, `filter()`, que generan nuevos RDDs.
- **Acciones**: operaciones como `collect()`, `count()`, que disparan la ejecuciÃ³n real.

---

#  Ejercicio paso a paso

## 1. Crear una SparkSession

```python
from pyspark.sql import SparkSession

# Crear la sesiÃ³n de Spark
spark = SparkSession.builder \
    .appName("PrimerRDD") \
    .getOrCreate()

# Obtener el contexto de Spark
sc = spark.sparkContext
```

---

## 2. Crear un RDD a partir de una lista

```python
# Crear un RDD con nÃºmeros del 1 al 10
numeros = sc.parallelize(range(1, 11))
```

> ğŸ”¸ `parallelize()` crea un RDD a partir de una lista o un rango de datos.

---

## 3. Aplicar transformaciones

```python
# TransformaciÃ³n: calcular el cuadrado de cada nÃºmero
numeros_cuadrado = numeros.map(lambda x: x * x)

# TransformaciÃ³n: filtrar solo los cuadrados mayores que 20
cuadrados_filtrados = numeros_cuadrado.filter(lambda x: x > 20)
```

> ğŸ”¸ `map()` y `filter()` son transformaciones: **no ejecutan nada todavÃ­a**.

---

## 4. Ejecutar acciones

```python
# AcciÃ³n: recolectar y mostrar los resultados
resultado = cuadrados_filtrados.collect()

print("Cuadrados mayores que 20:", resultado)
```

> ğŸ”¸ `collect()` dispara la ejecuciÃ³n y devuelve los resultados.

---

## 5. Cerrar la sesiÃ³n de Spark

```python
# Siempre cerrar la sesiÃ³n cuando termines
spark.stop()
```

---

# ğŸ“Š Resultado esperado

```
Cuadrados mayores que 20: [25, 36, 49, 64, 81, 100]
```

---

# ğŸ“ˆ Flujo visual del ejercicio

```
Crear RDD â†’ Transformar (map + filter) â†’ Ejecutar acciÃ³n (collect)
```

---

#  Comentarios importantes

- Las **transformaciones** son perezosas: no ejecutan inmediatamente.
- Las **acciones** fuerzan la ejecuciÃ³n real de las transformaciones acumuladas.
- Siempre **cierra la sesiÃ³n** de Spark para liberar recursos.

---

#  Siguiente paso

- TrabajarÃ¡s con **DataFrames**: estructuras mÃ¡s potentes y optimizadas para datos tabulares en PySpark.

---

#  Â¡Siguiente archivo: `03_pyspark_local_dataframes.md`! ğŸ“‚
