# 游늭 15 - Funciones Comunes en Azure Synapse Analytics (PySpark)

---

# Objetivo

Tener una referencia r치pida de las funciones **m치s comunes** que usar치s al trabajar con **PySpark** en **Azure Synapse Analytics**.

---

# Funciones de PySpark m치s usadas en Synapse

| Funci칩n | Uso | Ejemplo |
|:---|:---|:---|
| `read.format()` | Leer datos de diferentes formatos (Parquet, CSV, JSON) | `spark.read.format("parquet").load("ruta")` |
| `write.format()` | Guardar datos en diferentes formatos | `df.write.format("parquet").save("ruta")` |
| `select()` | Seleccionar columnas espec칤ficas | `df.select("columna1", "columna2")` |
| `filter()` | Filtrar filas basadas en condiciones | `df.filter(df.edad > 30)` |
| `withColumn()` | Crear o modificar columnas | `df.withColumn("nueva_col", df.edad + 5)` |
| `groupBy().agg()` | Agrupar datos y aplicar funciones de agregaci칩n | `df.groupBy("ciudad").agg(sum("ventas"))` |
| `orderBy()` | Ordenar los datos por una o m치s columnas | `df.orderBy("edad")` |
| `drop()` | Eliminar columnas | `df.drop("columna_a_eliminar")` |
| `dropDuplicates()` | Eliminar filas duplicadas | `df.dropDuplicates(["columna1"])` |
| `distinct()` | Eliminar duplicados en todo el DataFrame | `df.distinct()` |
| `limit()` | Limitar el n칰mero de filas | `df.limit(10)` |
| `cache()` | Cachear un DataFrame en memoria para acelerar procesos repetidos | `df.cache()` |
| `count()` | Contar el n칰mero de filas | `df.count()` |
| `show()` | Mostrar resultados en pantalla | `df.show()` |
| `printSchema()` | Mostrar la estructura del DataFrame | `df.printSchema()` |


---

# Funciones 칰tiles de `pyspark.sql.functions`

```python
from pyspark.sql.functions import col, sum, avg, count, max, min, when, lit
```

| Funci칩n | Uso |
|:---|:---|
| `col("nombre_columna")` | Referenciar columnas de manera segura. |
| `sum("columna")` | Sumar valores de una columna. |
| `avg("columna")` | Calcular promedio. |
| `count("columna")` | Contar valores. |
| `max("columna")` | Obtener el valor m치ximo. |
| `min("columna")` | Obtener el valor m칤nimo. |
| `when(condici칩n, valor).otherwise(valor)` | Equivalente a `IF-THEN-ELSE`. |
| `lit(valor)` | Crear una columna con un valor literal. |

---

# Notas importantes para Synapse

- Siempre adjunta el **Notebook** a un **Spark Pool** antes de ejecutar c칩digo.
- Si trabajas sobre un **Data Lake**, usa rutas tipo `"abfss://<filesystem>@<storageaccount>.dfs.core.windows.net/ruta"`.
- Cuando guardes datos, **usa `overwrite` o `append`** expl칤citamente para controlar c칩mo se escriben:

```python
df.write.mode("overwrite").parquet("ruta")
```

- **Evita usar `collect()`** sobre datasets muy grandes (puede saturar la memoria del driver).

---

# Consejo final

> *"Conocer estas funciones te permitir치 construir pipelines de datos eficientes en PySpark dentro de Synapse Analytics."*
