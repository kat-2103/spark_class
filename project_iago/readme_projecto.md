# üìä An√°lisis de Datos de Taxis - Proyecto Spark

Este repositorio contiene un notebook de Databricks que realiza una **limpieza, transformaci√≥n y an√°lisis exploratorio de datos** sobre un conjunto de archivos CSV relacionados con viajes en taxi amarillo (Yellow Taxi Trip Records).

## üìÅ Estructura del Proyecto

- `notebook.ipynb` - El notebook principal donde se ejecuta todo el procesamiento.
- `data/` - Carpeta donde se encuentran los archivos de entrada y salida (`csv/yellow_*.csv`, `csv/clean/`).
- `README.md` - Documentaci√≥n actual.

---

## üßæ Descripci√≥n del C√≥digo

### 1. üîç Carga de Datos
- Se cargan todos los archivos CSV (`yellow_*.csv`) desde la ruta especificada.
- Se infiere autom√°ticamente el esquema y se lee el encabezado.

### 2. üßπ Limpieza de Datos
- Se eliminan columnas irrelevantes o poco √∫tiles como: `airport_fee`, `RatecodeID`, `congestion_surcharge`, etc.
- Se identifican y manejan valores nulos reemplaz√°ndolos por la media de cada columna num√©rica:
  - Columnas afectadas: `passenger_count`, `trip_distance`, `fare_amount`, `total_amount`, `tip_amount`.
- Se convierten tipos de datos a formatos adecuados (ej. `DoubleType`, `IntegerType`).

## ‚ö†Ô∏è Nota sobre Outliers

En este an√°lisis **no se ha realizado una limpieza expl√≠cita de outliers**, por las siguientes razones:

- **Representatividad de casos reales**: muchos valores extremos (como viajes muy largos o tarifas altas) pueden representar situaciones leg√≠timas dentro del servicio de taxis.
- **Objetivo exploratorio del proyecto**: el enfoque est√° puesto en obtener una visi√≥n general de los patrones de uso, sin necesidad de eliminar datos para modelado predictivo.
- **Escalabilidad**: dado que los datos pueden ser muy grandes, la detecci√≥n y tratamiento de outliers puede implicar un costo computacional elevado si no se justifica por el objetivo del an√°lisis.
- **Posibilidad de an√°lisis posterior**: el tratamiento de outliers se deja pendiente para etapas posteriores, en caso de requerirse para modelos estad√≠sticos o de machine learning.

Si en una fase futura se requiere un an√°lisis m√°s riguroso o entrenamiento de modelos, se recomienda aplicar t√©cnicas como el rango intercuart√≠lico (IQR), z-score u otros m√©todos de detecci√≥n de anomal√≠as.

---
### 3. üïí Transformaciones Temporales
- Se convierten las columnas de fecha (`tpep_pickup_datetime`, `tpep_dropoff_datetime`) a tipo `Timestamp`.
- Se extrae la hora de ambas fechas (`hora_pickup`, `hora_dropoff`) y se convierten a enteros.
- Se eliminan las columnas temporales redundantes (`hora_pickup`, `hora_dropoff`).

### 4. ‚è±Ô∏è Duraci√≥n del Viaje
- Se crea una nueva columna `trip_duration_minutes` calculando la diferencia entre las horas de recogida y entrega.

### 5. üí≥ Mapeo de Tipos de Pago
- Se mapea el campo `payment_type` a nombres descriptivos (ej. "Tarjeta de cr√©dito", "Efectivo").
- Se une esta informaci√≥n al DataFrame original mediante un `join`.

### 6. üìÖ Fechas y Horarios
- Se extraen nuevas columnas:
  - `dia_semana`: d√≠a de la semana (1-7).
  - `nombre_dia`: nombre del d√≠a (ej. "Monday").
  - `mes`: n√∫mero del mes (1-12).
  - `nombre_mes`: nombre del mes (ej. "January").

### 7. üìà Agregaciones
Se generan varios DataFrames agrupados por hora de inicio del viaje (`hora_int_pickup`):
- **Pasajeros por hora**: total y promedio de pasajeros.
- **Viajes por hora**: distancia, duraci√≥n, tarifas promedio.
- **Ingresos por hora**: ingresos totales, cantidad de viajes y promedio por viaje.
- **Franjas horarias**: categorizaci√≥n en Ma√±ana, Tarde, Noche, etc., con agregaciones adicionales.

### 8. üì• Exportaci√≥n
- Los DataFrames limpios y agregados se exportan como archivos `.csv` en la carpeta `/Volumes/workspace/default/prueba/csv/clean/`.
- El DataFrame final tambi√©n se guarda como tabla Delta en Databricks (`workspace.default.df_final`).

---


## üóÇÔ∏è ¬øQu√© es Delta Lake?

[Delta Lake](https://delta.io/) es un proyecto open-source construido sobre Apache Spark que permite trabajar con datos estructurados y versionados en entornos de Big Data. Algunas ventajas clave de Delta Lake son:

- **Soporte ACID Transactions**: garantiza consistencia e integridad en escrituras concurrentes.
- **Metadatos mejorados**: facilita la gesti√≥n eficiente de grandes vol√∫menes de datos.
- **Versionado de datos**: permite retroceder en el tiempo a versiones anteriores del dataset.
- **Compatibilidad con Spark SQL**: integraci√≥n nativa con herramientas de consulta y procesamiento.

En este proyecto, se ha utilizado Delta Lake para almacenar el dataset final de forma estructurada y escalable, permitiendo su f√°cil acceso futuro para an√°lisis o integraci√≥n en pipelines.

---

## üìä Dashboards y Visualizaciones

Como parte del an√°lisis, se han creado **dashboards interactivos** para visualizar los siguientes **insights clave**:

| Insight | Descripci√≥n |
|--------|-------------|
| **Promedio de tarifa por hora del d√≠a** | Permite observar c√≥mo var√≠a la tarifa base seg√∫n la hora, lo cual puede estar influenciado por demanda o zonas de mayor actividad. |
| **Total de pasajeros por hora** | Muestra picos de uso del servicio en diferentes momentos del d√≠a, √∫til para planificaci√≥n operativa. |
| **Distancia media recorrida por hora** | Ayuda a entender patrones de movilidad urbana y destinos t√≠picos seg√∫n la hora. |
| **Total de tarifa por d√≠a de la semana** | Identifica d√≠as con mayores ingresos y comportamientos estacionales en el uso del servicio. |

Estos dashboards pueden implementarse utilizando herramientas como **Power BI**, **Tableau**, **Databricks SQL**, o incluso **Dash + Plotly** si se trabaja en un entorno Python.

---
## üìã Requisitos

Este proyecto est√° desarrollado en **Databricks** y utiliza **Apache Spark** con Python (PySpark). Para replicarlo necesitas:

- Acceso a un entorno Databricks.
- Archivos CSV de Yellow Taxi disponibles en la ruta especificada.
- Permisos para escribir en rutas de almacenamiento y crear tablas Delta.

---

## üîÑ Automatizaci√≥n con Triggers

Para hacer el proceso m√°s robusto y operativo, se han creado **dos triggers** dentro del entorno de Databricks:

### ‚úÖ Trigger de Ingesti√≥n de Datos
- **Objetivo:** Ejecutar autom√°ticamente la carga de nuevos archivos CSV cuando estos lleguen a la carpeta de origen.
- **Tipo:** *Event-based trigger* (por ejemplo, basado en la llegada de nuevos archivos a una ubicaci√≥n en DBFS o Cloud Storage).
- **Uso:** Ideal para mantener los datos actualizados sin intervenci√≥n manual cada vez que hay nuevos registros.

### ‚úÖ Trigger de Transformaci√≥n de Datos
- **Objetivo:** Ejecutar autom√°ticamente el proceso de limpieza, transformaci√≥n y agregaci√≥n una vez que se completa la ingesti√≥n.
- **Tipo:** *Scheduled trigger* (programado diariamente, semanalmente o bajo demanda).
- **Uso:** Permite refrescar los datasets limpios y las agregaciones para alimentar dashboards o reportes peri√≥dicos.

---
## ‚ö†Ô∏è Nota sobre Pipelines

En este proyecto **no se han implementado pipelines formales**, ya que:

- **Es una fase exploratoria**: el objetivo principal ha sido analizar y entender los datos, no crear una soluci√≥n operativa a largo plazo.
- **El volumen de datos es moderado**: a√∫n siendo Big Data, el alcance actual no requiere una arquitectura compleja ni orquestaci√≥n avanzada.
- **Se prioriz√≥ la simplicidad**: el uso de notebooks y triggers permite un desarrollo r√°pido y comprensible sin necesidad de m√∫ltiples herramientas intermedias.
- **No se requiere escalabilidad inmediata**: si el proyecto crece o se industrializa, entonces s√≠ ser√≠a recomendable migrar a un sistema de pipelines.
---

## üìå Resultados Generados

| DataFrame              | Descripci√≥n                                     |
|-----------------------|-------------------------------------------------|
| `df`                  | Dataset limpio y transformado listo para an√°lisis |
| `pasajeros_df`        | Total de pasajeros y viajes por hora             |
| `viajes_por_hora`     | Estad√≠sticas promedio por hora                   |
| `ingresos_por_hora`   | Ingresos por hora                                |
| `franjas_horarias`    | Ingresos divididos en franjas horarias          |
| `ingresos_por_franja` | Resumen de ingresos por franja horaria           |

---

## ‚úÖ Notas Adicionales

- Todas las transformaciones est√°n pensadas para ser escalables usando PySpark.
- Las estad√≠sticas son ideales para visualizar patrones de uso del servicio de taxis durante diferentes horas del d√≠a y d√≠as de la semana.
- Este proceso puede adaptarse f√°cilmente para incluir m√°s datasets o integrarse en pipelines automatizados.

---
