# Documentaci√≥n del Procesamiento de Datos de Taxis en Databricks

## ‚öîÔ∏è Introducci√≥n

Este documento describe el proceso de carga, limpieza, transformaci√≥n y almacenamiento de datos de **taxis amarrillos** en Databricks utilizando Delta Lake. El proyecto consisti√≥ en cargar datos de taxis desde archivos CSV, limpiarlos y transformarlos, y finalmente guardarlos en formato Delta para su posterior an√°lisis.

## ‚¨ÜÔ∏è 0. Subida de archivos csv locales a un volumen

Una vez extra√≠dos los datos a trav√©s del notebook [`extraccion.ipynb`](./extraccion.ipynb) subimos los 10 CSV al apartado de data ingestion de la web de Databricks.

## ü™´ 1. Carga de Datos

Primero, cargu√© los datos desde el volumen de Databricks que conten√≠a los archivos CSV de taxis:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# Ruta donde est√°n los archivos chunk_01.csv, chunk_02.csv, ...
input_path = "/Volumes/workspace/default/data-taxi/chunk_*.csv"

df = spark.read.option("header", True).option("inferSchema", True).csv(input_path)
```

Este paso me permiti√≥ cargar todos los archivos CSV disponibles en un √∫nico DataFrame para su procesamiento unificado.

## üëÄ 2. Visualizacion de los datos y posibles problemas

Despu√©s de cargar los datos, visualic√© los datos para analizar c√≥mo estaban estructurados y ver las transformaciones necesarias.

Visualizaciones b√°sicas:

```python
display(df)
df.count()
df.printSchema()
```

Visualizaci√≥n de Nulos:

```python
from pyspark.sql.functions import col, sum as _sum, when

null_counts = df.select([
    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)
    for c in df.columns
])
display(null_counts)
```

| VendorID | tpep_pickup_datetime | tpep_dropoff_datetime | passenger_count | trip_distance | RatecodeID | store_and_fwd_flag | PULocationID | DOLocationID | payment_type | payment_type | fare_amount | extra | mta_tax | tip_amount | tolls_amount | improvement_surcharge | total_amount | congestion_surcharge | airport_fee |
| -------- | -------------------- | --------------------- | --------------- | ------------- | ---------- | ------------------ | ------------ | ------------ | ------------ | ------------ | ----------- | ----- | ------- | ---------- | ------------ | --------------------- | ------------ | -------------------- | ----------- |
| 0        | 0                    | 0                     | 7602994         | 0             | 7602994    | 7602994            | 0            | 0            | 0            | 0            | 0           | 0     | 0       | 0          | 0            | 0                     | 0            | 7602994              | 35604821    |

Visualizaci√≥n de Outliers:

```python
# Seleccionar solo columnas num√©ricas
numeric_cols = [col for col, dtype in df.dtypes if dtype in ("double", "int")]

# Mostrar lista si quieres verla
print("Columnas num√©ricas:", numeric_cols)

# Muestreo del 1%
sample_df = df.select(numeric_cols).sample(0.01, seed=42).toPandas()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(15, 6))
sns.boxplot(data=sample_df[numeric_cols])
plt.title("Distribuci√≥n de columnas num√©ricas (muestra 1%)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

![Outliers](./images/outliers.png)

## üßπ 3. Limpieza y Transformaci√≥n de Datos

Despu√©s de cargar los datos, realic√© varias transformaciones para limpiarlos:

```python
df_clean = (
    df.withColumn("pickup_datetime", to_timestamp(col("tpep_pickup_datetime")))
    .withColumn("dropoff_datetime", to_timestamp(col("tpep_dropoff_datetime")))
    .filter(col("pickup_datetime").isNotNull())
    .filter(col("dropoff_datetime").isNotNull())
    .filter(col("passenger_count") > 0)
    .filter(col("trip_distance") > 0)
    .filter(col("fare_amount") >= 0)
    .withColumn("pickup_hour", hour(col("pickup_datetime")))
)

# Eliminar columna 'ratecodeid' y 'vendorid' ya que son identificadores y no aportan informaci√≥n interesante
df = df.drop('ratecodeid')
df = df.drop('vendorid')
df = df.drop('airport_fee')
```

Las operaciones de limpieza incluyeron:

- Conversi√≥n de fechas a formato timestamp
- Eliminaci√≥n de registros con fechas nulas
- Filtrado de registros con valores inv√°lidos (pasajeros <= 0, distancia <= 0, tarifa < 0)
- Extracci√≥n de la hora de recogida para an√°lisis temporal
- Eliminaci√≥n de columnas innecesarias

## üîé 4. Tratamiento de Outliers y Valores Nulos

Para controlar los Outliers no pude evaluarlos adecuadamente debido al gran volumen de datos, solo pude visualizar una muestra del 1%, lo cual dificult√≥ su evaluaci√≥n de los datos, por esto solo trabaje los nulos de la unica columna que se veia claramente que tenia muchos valores nulos

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

columna = "trip_distance"
# Calcular Q1 y Q3
quantiles = df.approxQuantile(columna , [0.25, 0.75], 0.01)
q1, q3 = quantiles[0], quantiles[1]
iqr = q3 - q1

# Definir l√≠mites inferior y superior
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

# Filtrar los outliers
df_sin_outliers = df.filter((col(columna) >= lower_bound) & (col(columna) <= upper_bound))
```

![Outliers](./images/sin_outliers.png)

Se podr√≠a haber realizado un estudio de los outliers mucho mas complejo pero ante la falta de tiempo y la limitaci√≥n de la web decid√≠ hacer un analisis basico y simple.

Para manejar los valores nulos, simplemente borre todo tipo de nulos ya que la cantidad de datos era lo suficientemente grande para permitirmelo:

1. Eliminar nulos en todas las columnas

```python
df_limpio = df_sin_outliers.dropna()
```

Elimin√© todos los registros con valores nulos ya que con tanta cantidad de datos era dif√≠cil evaluar si realmente eran esenciales.

## ‚ûï 5. Creaci√≥n de Agregados

*(Es necesario haber creado previamente la columna 'pickup_hour')*

```python
from pyspark.sql.functions import col, to_timestamp, hour

# Limpieza b√°sica
df_limpio = df.withColumn("pickup_datetime", to_timestamp(col("tpep_pickup_datetime"))).withColumn("dropoff_datetime", to_timestamp(col("tpep_dropoff_datetime"))).filter(col("pickup_datetime").isNotNull()).filter(col("passenger_count") > 0)

# Nueva columna con la hora de recogida
df_limpio = df_limpio.withColumn("pickup_hour", hour("pickup_datetime"))
```

Para facilitar los an√°lisis comunes, cre√© un DataFrame agregado por hora:

```python
agg_df = df_limpio.groupBy("pickup_hour").agg({"fare_amount": "avg", "trip_distance": "avg", "passenger_count": "sum"}).withColumnRenamed("avg(fare_amount)", "avg_fare").withColumnRenamed("avg(trip_distance)", "avg_distance").withColumnRenamed("sum(passenger_count)", "total_passengers")
```

Este DataFrame agregado me permite analizar r√°pidamente patrones por hora del d√≠a sin necesidad de consultar todo el conjunto de datos.

## üåä 6. Almacenamiento en Delta Lake

Guard√© tanto el DataFrame limpio como los agregados en formato Delta:

```python
agg_df.write.format("delta").mode("overwrite").saveAsTable("agg_df_delta")
df_limpio.write.format("delta").mode("overwrite").saveAsTable("df_limpio_delta")
```

Delta Lake me proporciona varias ventajas:

- ACID transactions
- Historial de versiones (Time Travel)
- Optimizaciones de rendimiento
- Aplicaci√≥n de esquema

## üëå 7. Preparaci√≥n para Ingesta Incremental

Finalmente, prepar√© funciones para la ingesta incremental de nuevos datos:

```python
def procesar_e_ingerir_datos(ruta_origen, tabla_destino):
    # Leer nuevos datos
    nuevos_datos = spark.read.option("header", True).schema(schema).csv(ruta_origen)
  
    # Aplicar las mismas transformaciones
    nuevos_datos_procesados = (
        nuevos_datos.withColumn("pickup_datetime", to_timestamp(col("tpep_pickup_datetime")))
        .withColumn("dropoff_datetime", to_timestamp(col("tpep_dropoff_datetime")))
        .filter(col("pickup_datetime").isNotNull())
        .filter(col("passenger_count") > 0)
        .filter(col("trip_distance") > 0)
        .filter(col("fare_amount") >= 0)
        .withColumn("pickup_hour", hour("pickup_datetime"))
    )
  
    # Guardar en formato Delta (modo append)
    nuevos_datos_procesados.write \
        .format("delta") \
        .mode("append") \
        .saveAsTable(tabla_destino)
  
    return nuevos_datos_procesados.count()
```

Tambi√©n implement√© funciones para verificar la calidad de los datos y compactar el historial de logs, proporcionando un pipeline completo para mantenimiento continuo de datos.

## üöÄ 8. Visualizaciones

Finalmente realic√© algunas visualizaciones una vez procesado todos los datos, estas visualizaciones son las que luego se mostraran en el dashboard.

### üìä 1. Promedio de tarifa por hora del d√≠a

![Outliers](./images/1.png)

### üìä 2. Total de pasajeros por hora

![Outliers](./images/2.png)

### üìä 3. Distancia media recorrida por hora

![Outliers](./images/3.png)

## Conclusiones

Gracias a este proceso, pude manejar de forma eficiente un gran conjunto de datos de taxis y almacenarlos en un formato optimizado para an√°lisis. La integraci√≥n con Delta Lake aporta m√∫ltiples beneficios cuando se trabaja a gran escala, tales como:

- Gesti√≥n de versiones de los datos
- Mejoras en el rendimiento de consultas
- Mantenimiento automatizado
- Enforzamiento del esquema de datos

Con estos pasos completados, los datos est√°n preparados para an√°lisis m√°s profundos, creaci√≥n de visualizaciones y potencial uso en modelos de machine learning.
