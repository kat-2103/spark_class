# Documentaci√≥n del Procesamiento de Datos de Taxis en Databricks

## ‚öîÔ∏è Introducci√≥n

Este documento describe el proceso de carga, limpieza, transformaci√≥n y almacenamiento de datos de **taxis amarrillos** en Databricks utilizando Delta Lake. El proyecto consisti√≥ en cargar datos de taxis desde archivos CSV, limpiarlos y transformarlos, y finalmente guardarlos en formato Delta para su posterior an√°lisis.

## ‚¨ÜÔ∏è 0. Subida de archivos csv locales a un volumen

Una vez extra√≠dos los datos a trav√©s del notebook [`extraccion.ipynb`](./extraccion.ipynb) subimos los 10 CSV a un volumen dentro del apartado de data ingestion de la web de Databricks.

## ü™´ 1. Carga de Datos

Primero, cargu√© los datos desde el volumen de Databricks que conten√≠a los archivos CSV de taxis:

```python
from pyspark.sql import SparkSession

# Crea una sesi√≥n de Spark o reutiliza una existente.
spark = SparkSession.builder.getOrCreate()

# Ruta donde est√°n los archivos chunk_01.csv, chunk_02.csv, ...
input_path = "/Volumes/workspace/default/data-taxi/chunk_*.csv"

# Lee los archivos CSV como un solo DataFrame
df = spark.read.option("header", True).option("inferSchema", True).csv(input_path)
```

Este paso me permiti√≥ cargar todos los archivos CSV disponibles en un √∫nico DataFrame para su procesamiento unificado.

## üëÄ 2. Visualizacion de los datos y posibles problemas

Despu√©s de cargar los datos, visualic√© los datos para analizar c√≥mo estaban estructurados y ver las transformaciones necesarias.

Visualizaciones b√°sicas:

```python
display(df) # Muestra el contenido del DataFrame de forma tabular en un entorno interactivo como Databricks o Jupyter.
df.count() # Devuelve el n√∫mero total de filas (registros) en el DataFrame.
df.printSchema() # Muestra la estructura del DataFrame, incluyendo los nombres de las columnas, sus tipos de datos y si pueden ser nulas.
```

Visualizaci√≥n de Nulos:

```python
from pyspark.sql.functions import col, sum as _sum, when

# Calcula la cantidad de valores nulos por columna en el DataFrame.
# Para cada columna `c`, se aplica una condici√≥n que eval√∫a si el valor es nulo.
# Si es nulo, cuenta 1; de lo contrario, cuenta 0. Luego, se suman los resultados por columna.
# El resultado es un DataFrame con una sola fila donde cada columna indica cu√°ntos valores nulos tiene esa columna original.
null_counts = df.select([
    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)
    for c in df.columns
])

# Muestra el DataFrame resultante que contiene el conteo de nulos por cada columna del DataFrame original.
display(null_counts)
```

| VendorID | tpep_pickup_datetime | tpep_dropoff_datetime | passenger_count | trip_distance | RatecodeID | store_and_fwd_flag | PULocationID | DOLocationID | payment_type | payment_type | fare_amount | extra | mta_tax | tip_amount | tolls_amount | improvement_surcharge | total_amount | congestion_surcharge | airport_fee |
| -------- | -------------------- | --------------------- | --------------- | ------------- | ---------- | ------------------ | ------------ | ------------ | ------------ | ------------ | ----------- | ----- | ------- | ---------- | ------------ | --------------------- | ------------ | -------------------- | ----------- |
| 0        | 0                    | 0                     | 7602994         | 0             | 7602994    | 7602994            | 0            | 0            | 0            | 0            | 0           | 0     | 0       | 0          | 0            | 0                     | 0            | 7602994              | 35604821    |

Visualizaci√≥n de Outliers:

```python
# Seleccionar solo columnas num√©ricas
numeric_cols = [col for col, dtype in df.dtypes if dtype in ("double", "int")]

# Mostrar lista si quieres verla
print("Columnas num√©ricas:", numeric_cols)

# Muestreo del 1%
sample_df = df.select(numeric_cols).sample(0.01, seed=42).toPandas()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(15, 6))
sns.boxplot(data=sample_df[numeric_cols])
plt.title("Distribuci√≥n de columnas num√©ricas (muestra 1%)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

![Outliers](./images/outliers.png)

## üßπ 3. Limpieza y Transformaci√≥n de Datos

Despu√©s de cargar los datos, realic√© varias transformaciones para limpiarlos:

```python
df = (
    df.filter(col("tpep_pickup_datetime").isNotNull())
    .filter(col("tpep_dropoff_datetime").isNotNull())
    .filter(col("passenger_count") > 0)
    .filter(col("trip_distance") > 0)
    .filter(col("fare_amount") >= 0)
)

# Eliminar columna 'ratecodeid', 'vendorid' y 'airport_fee' ya que son identificadores y no aportan informaci√≥n interesante
df = df.drop('ratecodeid')
df = df.drop('vendorid')
df = df.drop('airport_fee')
```

Las operaciones de limpieza incluyeron:

- Eliminaci√≥n de registros con fechas nulas
- Filtrado de registros con valores inv√°lidos (pasajeros <= 0, distancia <= 0, tarifa < 0)
- Eliminaci√≥n de columnas innecesarias

## üîé 4. Tratamiento de Outliers y Valores Nulos

Para controlar los Outliers no pude evaluarlos adecuadamente debido al gran volumen de datos, solo pude visualizar una muestra del 1%, lo cual dificult√≥ su evaluaci√≥n de los datos, por esto solo trabaje los nulos de la unica columna que se veia claramente que tenia muchos valores nulos

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

columna = "trip_distance"
# Calcular Q1 y Q3
quantiles = df.approxQuantile(columna , [0.25, 0.75], 0.01)
q1, q3 = quantiles[0], quantiles[1]
iqr = q3 - q1

# Definir l√≠mites inferior y superior
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

# Filtrar los outliers
df_sin_outliers = df.filter((col(columna) >= lower_bound) & (col(columna) <= upper_bound))
```

![Outliers](./images/sin_outliers.png)

Se podr√≠a haber realizado un estudio de los outliers mucho mas complejo pero ante la falta de tiempo y la limitaci√≥n de la web decid√≠ hacer un analisis basico y simple.

Para manejar los valores nulos, simplemente borre todo tipo de nulos ya que la cantidad de datos era lo suficientemente grande para permitirmelo:

1. Eliminar nulos en todas las columnas

```python
df_limpio = df_sin_outliers.dropna()
```

Elimin√© todos los registros con valores nulos ya que con tanta cantidad de datos era dif√≠cil evaluar si realmente eran esenciales.

## ‚ûï 5. Creaci√≥n de Agregados

*(Es necesario haber creado previamente la columna 'pickup_hour' que almacena las horas del dia en tipo string)*

```python
from pyspark.sql.functions import col, to_timestamp, hour

# Limpieza b√°sica del DataFrame:
# 1. Convierte las columnas de fecha y hora de recogida y entrega de tipo string a tipo timestamp.
# 2. Elimina las filas donde la columna de recogida es nula.
# 3. Filtra solo los registros con al menos un pasajero.
df_limpio = (
    df.withColumn("pickup_datetime", to_timestamp(col("tpep_pickup_datetime")))
      .withColumn("dropoff_datetime", to_timestamp(col("tpep_dropoff_datetime")))
      .filter(col("pickup_datetime").isNotNull())
      .filter(col("passenger_count") > 0)
)

# Agrega una nueva columna `pickup_hour` que extrae la hora (en formato 0‚Äì23) del timestamp de recogida.
df_limpio = df_limpio.withColumn("pickup_hour", hour("pickup_datetime"))

```

Para facilitar los an√°lisis comunes, cre√© un DataFrame agregado por hora:

```python
agg_df = df_limpio.groupBy("pickup_hour").agg({"fare_amount": "avg", "trip_distance": "avg", "passenger_count": "sum"}).withColumnRenamed("avg(fare_amount)", "avg_fare").withColumnRenamed("avg(trip_distance)", "avg_distance").withColumnRenamed("sum(passenger_count)", "total_passengers")
```

Este DataFrame agregado me permite analizar r√°pidamente patrones por hora del d√≠a sin necesidad de consultar todo el conjunto de datos.

## üåä 6. Almacenamiento en Delta Lake

Guard√© tanto el DataFrame limpio como los agregados en formato Delta:

```python
agg_df.write.format("delta").mode("overwrite").saveAsTable("agg_df_delta")
df_limpio.write.format("delta").mode("overwrite").saveAsTable("df_limpio_delta")
```

Delta Lake me proporciona varias ventajas:

- ACID transactions
- Historial de versiones (Time Travel)
- Optimizaciones de rendimiento
- Aplicaci√≥n de esquema

## üöÄ 7. Visualizaciones

Finalmente realic√© algunas visualizaciones una vez procesado todos los datos, estas visualizaciones son las que luego se mostraran en el dashboard.

### üìä 1. Promedio de tarifa por hora del d√≠a

![Outliers](./images/1.png)

### üìä 2. Total de pasajeros por hora

![Outliers](./images/2.png)

### üìä 3. Distancia media recorrida por hora

![Outliers](./images/3.png)

## ‚ö†Ô∏è Mejoras por implementar

Dado a la falta de tiempo qued√≥ por implementar varias funciones que mejorarian la automatizaci√≥n de este proyecto, las cuales se podrian aplicar en un futuro, estas son:

* Ingesta de M√∫ltiples Archivos y Pipeline Incremental
* Automatizaci√≥n con Triggers

## ü´µ Conclusiones

Gracias a este proceso, pude manejar de forma eficiente un gran conjunto de datos de taxis y almacenarlos en un formato optimizado para an√°lisis. La integraci√≥n con Delta Lake aporta m√∫ltiples beneficios cuando se trabaja a gran escala, los puntos aprendidos en este proyecto fueron:

- Procesamiento eficiente de grandes vol√∫menes de datos
- Limpieza y preparaci√≥n de datos exitosa
- Almacenamiento optimizado con Delta Lake
- Visualizacion de los datos

Con estos pasos completados, los datos est√°n preparados para an√°lisis m√°s profundos, creaci√≥n de visualizaciones y potencial uso en modelos de machine learning.
